# =============================================================================
# RAG API Kubernetes Manifests
# =============================================================================
# Deploys a FastAPI-based RAG service using Qdrant + Ollama
# No custom image needed - uses public python:3.11-slim with ConfigMap
# =============================================================================

---
# NOTE: Create this secret manually with the Qdrant API key:
#
# 1. Get the actual key from the Qdrant pod:
#    kubectl exec -n ai-inference qdrant-0 -- cat /qdrant/config/local.yaml
#
# 2. Create the secret:
#    kubectl create secret generic rag-api-qdrant-key \
#      --from-literal=api-key="<key-from-step-1>" \
#      -n ai-inference
#
# Or use SealedSecrets for GitOps (recommended)
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-api-config
  namespace: ai-inference
  labels:
    app: rag-api
    app.kubernetes.io/name: rag-api
    app.kubernetes.io/component: config
data:
  # Environment configuration
  QDRANT_URL: "http://qdrant.ai-inference.svc.cluster.local:6333"
  OLLAMA_URL: "http://ollama.ai-inference.svc.cluster.local:11434"
  QDRANT_COLLECTION: "documents"
  EMBEDDING_MODEL: "nomic-embed-text"
  LLM_MODEL: "mistral:7b-instruct-v0.3-q4_K_M"
  CHUNK_SIZE: "1000"
  CHUNK_OVERLAP: "100"
  TOP_K: "3"
  PORT: "8000"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-api-script
  namespace: ai-inference
  labels:
    app: rag-api
    app.kubernetes.io/name: rag-api
    app.kubernetes.io/component: script
data:
  requirements.txt: |
    fastapi>=0.109.0
    uvicorn>=0.27.0
    requests>=2.31.0
    pydantic>=2.5.0

  startup.sh: |
    #!/bin/bash
    set -e
    
    echo "ðŸ“¦ Installing dependencies..."
    pip install --no-cache-dir -q -r /app/requirements.txt
    
    echo "ðŸš€ Starting RAG API..."
    cd /app
    exec python rag_api.py serve

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-api
  namespace: ai-inference
  labels:
    app: rag-api
    app.kubernetes.io/name: rag-api
    app.kubernetes.io/component: api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rag-api
  template:
    metadata:
      labels:
        app: rag-api
        app.kubernetes.io/name: rag-api
    spec:
      containers:
      - name: rag-api
        image: python:3.11-slim
        command: ["/bin/bash", "/app/startup.sh"]
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        envFrom:
        - configMapRef:
            name: rag-api-config
        env:
        - name: QDRANT_API_KEY
          valueFrom:
            secretKeyRef:
              name: rag-api-qdrant-key
              key: api-key
        volumeMounts:
        - name: script
          mountPath: /app/rag_api.py
          subPath: rag_api.py
        - name: startup
          mountPath: /app/startup.sh
          subPath: startup.sh
        - name: startup
          mountPath: /app/requirements.txt
          subPath: requirements.txt
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: script
        configMap:
          name: rag-api-code
      - name: startup
        configMap:
          name: rag-api-script
          defaultMode: 0755

---
apiVersion: v1
kind: Service
metadata:
  name: rag-api
  namespace: ai-inference
  labels:
    app: rag-api
    app.kubernetes.io/name: rag-api
spec:
  type: ClusterIP
  selector:
    app: rag-api
  ports:
  - name: http
    port: 8000
    targetPort: 8000
    protocol: TCP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rag-api
  namespace: ai-inference
  labels:
    app: rag-api
    app.kubernetes.io/name: rag-api
  annotations:
    cert-manager.io/cluster-issuer: "ai-platform-ca-issuer"
    traefik.ingress.kubernetes.io/router.tls: "true"
spec:
  ingressClassName: traefik
  tls:
  - hosts:
    - rag.ai-platform.localhost
    secretName: rag-api-tls
  rules:
  - host: rag.ai-platform.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: rag-api
            port:
              number: 8000
