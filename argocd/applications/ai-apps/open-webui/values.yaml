ollamaUrls:
  - "http://ollama.ai-inference.svc:11434"
ollama:
  enabled: false
postgresql:
  enabled: false
ingress:
  enabled: true
  class: traefik
  annotations:
    cert-manager.io/cluster-issuer: ai-platform-ca-issuer
  host: chat.ai-platform.localhost
  tls: true
persistence:
  enabled: true
  size: 5Gi
  storageClass: local-path
resources:
  requests:
    memory: "512Mi"
    cpu: "200m"
  limits:
    memory: "2Gi"
    cpu: "1000m"
extraEnvVars:
  - name: WEBUI_NAME
    value: "AI Security Platform"
  - name: ENABLE_SIGNUP
    value: "false"
  - name: DEFAULT_MODELS
    value: "mistral:7b-instruct-v0.3-q4_K_M"
  - name: DATABASE_URL
    value: "postgresql://openwebui:openwebui123@postgresql-cluster-rw.storage.svc:5432/openwebui"
  # OIDC Configuration
  - name: ENABLE_OAUTH_SIGNUP
    value: "true"
  - name: OAUTH_MERGE_ACCOUNTS_BY_EMAIL
    value: "true"
  - name: DEFAULT_USER_ROLE
    value: "user"
  - name: OAUTH_PROVIDER_NAME
    value: "Keycloak"
  - name: OPENID_PROVIDER_URL
    value: "http://keycloak-keycloakx-http.auth.svc/realms/ai-platform/.well-known/openid-configuration"
  - name: OAUTH_CLIENT_ID
    value: "open-webui"
  - name: OAUTH_CLIENT_SECRET
    valueFrom:
      secretKeyRef:
        name: openwebui-oidc-secret
        key: client-secret
  - name: OAUTH_SCOPES
    value: "openid email profile"
  - name: OPENID_REDIRECT_URI
    value: "https://chat.ai-platform.localhost/oauth/oidc/callback"

# =============================================================================
# Pipelines Configuration (LLM Guard + RAG)
# Kyverno-compliant security settings
# =============================================================================
pipelines:
  enabled: true
  
  # Container security context (Kyverno: disallow-privileged, require-non-root)
  containerSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    privileged: false
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: false
    capabilities:
      drop:
        - ALL
    seccompProfile:
      type: "RuntimeDefault"
  
  # Resource limits (Kyverno: require-resource-limits / OWASP LLM04)
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"
  
  # Persistence for pipelines
  persistence:
    enabled: true
    size: 1Gi
    storageClass: local-path
  
  # Extra environment variables for pipeline configuration
  extraEnvVars:
    - name: PIPELINES_URLS
      value: "http://guardrails-api.ai-inference.svc.cluster.local:8000"
