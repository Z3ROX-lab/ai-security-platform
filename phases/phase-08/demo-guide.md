# Phase 8: Observability - Demo Guide

## Overview

Ce guide fournit des exemples de requ√™tes et sc√©narios de d√©monstration pour la stack observability.

## URLs

| Service | URL | Credentials |
|---------|-----|-------------|
| **Grafana** | https://grafana.ai-platform.localhost | admin / admin123! |
| **Prometheus** | https://prometheus.ai-platform.localhost | - |
| **Alertmanager** | https://alertmanager.ai-platform.localhost | - |

---

## D√©mo 1: M√©triques Cluster (Prometheus)

### Objectif
Visualiser les m√©triques du cluster Kubernetes.

### Via Grafana Dashboard

1. Ouvrir https://grafana.ai-platform.localhost
2. **Dashboards** ‚Üí Chercher "Kubernetes / Compute Resources / Cluster"
3. Observer : CPU, Memory, Network par namespace

### Via Prometheus directement

1. Ouvrir https://prometheus.ai-platform.localhost
2. **Graph** ‚Üí Entrer une requ√™te

### Exemples de requ√™tes PromQL

#### CPU Usage par namespace

```promql
sum(rate(container_cpu_usage_seconds_total{namespace!=""}[5m])) by (namespace)
```

#### Memory Usage par namespace

```promql
sum(container_memory_working_set_bytes{namespace!=""}) by (namespace) / 1024 / 1024 / 1024
```

#### Pods Running par namespace

```promql
count(kube_pod_status_phase{phase="Running"}) by (namespace)
```

#### CPU Usage des LLM components

```promql
sum(rate(container_cpu_usage_seconds_total{namespace=~"ai-inference|ai-apps"}[5m])) by (pod)
```

#### Memory des Guardrails

```promql
container_memory_working_set_bytes{namespace="ai-inference", pod=~"guardrails.*"} / 1024 / 1024
```

---

## D√©mo 2: Logs Application (Loki)

### Objectif
Explorer les logs des applications AI.

### Via Grafana Explore

1. Ouvrir https://grafana.ai-platform.localhost
2. **Explore** (ic√¥ne compas)
3. S√©lectionner **Loki** en haut
4. Entrer une requ√™te LogQL

### Exemples de requ√™tes LogQL

#### Logs Open WebUI

```logql
{namespace="ai-apps", app="open-webui"}
```

#### Logs Pipelines (Guardrails filter)

```logql
{namespace="ai-apps", pod=~"open-webui-pipelines.*"}
```

#### Logs LLM Guard avec filtrage

```logql
{namespace="ai-apps"} |= "LLM Guard"
```

#### Logs Guardrails API

```logql
{namespace="ai-inference", app="guardrails-api"}
```

#### Logs Ollama (LLM)

```logql
{namespace="ai-inference", app="ollama"}
```

#### Logs RAG API

```logql
{namespace="ai-inference", app="rag-api"}
```

#### Erreurs uniquement

```logql
{namespace="ai-inference"} |= "error" or |= "Error" or |= "ERROR"
```

#### Logs avec JSON parsing

```logql
{namespace="ai-inference", app="rag-api"} | json | line_format "{{.level}} {{.message}}"
```

#### Rate de logs par pod

```logql
sum(rate({namespace="ai-apps"}[5m])) by (pod)
```

---

## D√©mo 3: Monitoring LLM en temps r√©el

### Objectif
Surveiller les performances du LLM pendant une requ√™te.

### Setup

Terminal 1 - Watch logs :
```bash
kubectl logs -n ai-apps deployment/open-webui-pipelines -f | grep "LLM Guard"
```

Terminal 2 - Watch metrics :
```bash
watch kubectl top pods -n ai-inference
```

### Test

1. Ouvrir https://chat.ai-platform.localhost
2. Envoyer : "What is Kubernetes?"
3. Observer les logs et m√©triques

### Dans Grafana

1. **Explore** ‚Üí **Loki**
2. Query : `{namespace="ai-apps"} |= "LLM Guard"`
3. Cliquer **Live** (en haut √† droite)
4. Envoyer des messages dans le chat
5. Voir les logs en temps r√©el

---

## D√©mo 4: Prometheus - M√©triques & Alertes

### Objectif
Explorer Prometheus UI, tester des requ√™tes et comprendre les alertes.

### 4.1 Explorer Prometheus UI

1. Ouvrir https://prometheus.ai-platform.localhost
2. Les onglets principaux :
   - **Graph** : Ex√©cuter des requ√™tes PromQL
   - **Alerts** : Voir les r√®gles d'alertes et leur √©tat
   - **Status > Targets** : Voir ce que Prometheus scrape
   - **Status > Configuration** : Config active

### 4.2 V√©rifier les Targets

1. https://prometheus.ai-platform.localhost/targets
2. Tous les endpoints doivent √™tre **UP** (vert)
3. Si un target est **DOWN** (rouge) ‚Üí probl√®me de scraping

| Target | Description |
|--------|-------------|
| `kubernetes-apiservers` | API Kubernetes |
| `kubernetes-nodes` | M√©triques kubelet |
| `kubernetes-pods` | Pods avec annotations prometheus |
| `node-exporter` | M√©triques syst√®me (CPU, RAM, Disk) |
| `kube-state-metrics` | √âtat des objets K8s |

### 4.3 Tester des requ√™tes PromQL

1. https://prometheus.ai-platform.localhost/graph
2. Entrer une requ√™te ‚Üí **Execute** ‚Üí Voir r√©sultat (Table ou Graph)

#### Requ√™tes de base

```promql
# Nombre de pods running par namespace
count(kube_pod_status_phase{phase="Running"}) by (namespace)

# Usage CPU total du cluster (%)
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Usage m√©moire par node (GB)
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / 1024 / 1024 / 1024

# Pods en restart (derni√®re heure)
increase(kube_pod_container_status_restarts_total[1h]) > 0
```

#### Requ√™tes AI Platform sp√©cifiques

```promql
# CPU des composants AI
sum(rate(container_cpu_usage_seconds_total{namespace=~"ai-inference|ai-apps"}[5m])) by (pod)

# M√©moire Ollama (MB)
container_memory_working_set_bytes{namespace="ai-inference", pod=~"ollama.*"} / 1024 / 1024

# M√©moire Guardrails (MB)
container_memory_working_set_bytes{namespace="ai-inference", pod=~"guardrails.*"} / 1024 / 1024

# Pods not ready
kube_pod_status_ready{condition="false"}
```

### 4.4 Comprendre les √©tats d'alertes

1. https://prometheus.ai-platform.localhost/alerts
2. Les 3 √©tats possibles :

| √âtat | Couleur | Signification |
|------|---------|---------------|
| **Inactive** | üü¢ Vert | Condition non remplie, tout va bien |
| **Pending** | üü° Jaune | Condition remplie, attente de confirmation (for: duration) |
| **Firing** | üî¥ Rouge | Alerte confirm√©e, envoy√©e √† Alertmanager |

### 4.5 Alertes pr√©-configur√©es importantes

| Alerte | S√©v√©rit√© | Description |
|--------|----------|-------------|
| **Watchdog** | info | Toujours "Firing" - prouve que le syst√®me fonctionne |
| **KubePodCrashLooping** | warning | Pod restart en boucle |
| **KubePodNotReady** | warning | Pod non pr√™t > 15min |
| **KubeDeploymentReplicasMismatch** | warning | Replicas attendus ‚â† r√©els |
| **NodeFilesystemSpaceFillingUp** | warning | Disque > 80% |
| **NodeMemoryHighUtilization** | warning | RAM > 90% |
| **PrometheusTargetMissing** | warning | Target de scraping down |

### 4.6 Requ√™tes pour voir les alertes

```promql
# Toutes les alertes qui "fire"
ALERTS{alertstate="firing"}

# Alertes pending
ALERTS{alertstate="pending"}

# Alertes par s√©v√©rit√©
ALERTS{severity="critical"}
ALERTS{severity="warning"}

# Alertes d'un namespace sp√©cifique
ALERTS{namespace="ai-inference"}
```

---

## D√©mo 5: Alertmanager - Gestion des alertes

### Objectif
Comprendre Alertmanager et tester le syst√®me d'alertes.

### 5.1 Explorer Alertmanager UI

1. Ouvrir https://alertmanager.ai-platform.localhost
2. Les sections :
   - **Alerts** : Alertes actives re√ßues de Prometheus
   - **Silences** : Alertes mises en silence
   - **Status** : Configuration et √©tat

### 5.2 Architecture des alertes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Alerte      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Notification
‚îÇ Prometheus ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ Alertmanager ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫
‚îÇ            ‚îÇ                 ‚îÇ              ‚îÇ
‚îÇ ‚Ä¢ D√©tecte  ‚îÇ                 ‚îÇ ‚Ä¢ Groupe     ‚îÇ    ‚Ä¢ Email
‚îÇ ‚Ä¢ √âvalue   ‚îÇ                 ‚îÇ ‚Ä¢ D√©duplique ‚îÇ    ‚Ä¢ Slack
‚îÇ ‚Ä¢ Envoie   ‚îÇ                 ‚îÇ ‚Ä¢ Route      ‚îÇ    ‚Ä¢ PagerDuty
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚Ä¢ Webhook
```

### 5.3 V√©rifier les alertes actives

```bash
# Via curl (JSON)
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq .

# Alertes actives (noms seulement)
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq '.[].labels.alertname'

# Compter les alertes
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq 'length'
```

### 5.4 Test 1 : Envoyer une alerte manuelle

```bash
# Cr√©er une alerte de test
curl -sk -X POST https://alertmanager.ai-platform.localhost/api/v2/alerts \
  -H "Content-Type: application/json" \
  -d '[{
    "labels": {
      "alertname": "TestAlert",
      "severity": "warning",
      "namespace": "demo",
      "service": "test-service"
    },
    "annotations": {
      "summary": "Ceci est une alerte de test",
      "description": "Test manuel de Alertmanager"
    },
    "generatorURL": "https://prometheus.ai-platform.localhost"
  }]'

# V√©rifier dans UI : https://alertmanager.ai-platform.localhost
# L'alerte "TestAlert" devrait appara√Ætre

# V√©rifier via API
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq '.[] | select(.labels.alertname=="TestAlert")'
```

### 5.5 Test 2 : D√©clencher une vraie alerte Kubernetes

```bash
# STEP 1: Scale down un deployment
kubectl scale deployment -n ai-apps open-webui-pipelines --replicas=0

echo "Attendre 2-5 minutes pour que l'alerte se d√©clenche..."
echo "Observer: https://prometheus.ai-platform.localhost/alerts"
echo "          ‚Üí KubeDeploymentReplicasMismatch devrait passer en Pending puis Firing"

# STEP 2: V√©rifier dans Prometheus (apr√®s 2 min)
# https://prometheus.ai-platform.localhost/alerts
# Chercher: KubeDeploymentReplicasMismatch ‚Üí √©tat "Pending" ou "Firing"

# STEP 3: V√©rifier dans Alertmanager (apr√®s 5 min)
# https://alertmanager.ai-platform.localhost
# L'alerte devrait appara√Ætre

# STEP 4: RESTAURER !
kubectl scale deployment -n ai-apps open-webui-pipelines --replicas=1

# STEP 5: Observer l'alerte dispara√Ætre
# Dans Prometheus: √©tat revient √† "Inactive"
# Dans Alertmanager: alerte r√©solue
```

### 5.6 Test 3 : Simuler un pod crash

```bash
# Tuer un pod pour simuler un crash
kubectl delete pod -n ai-apps -l app.kubernetes.io/name=promtail --force

# Observer:
# - Le pod restart automatiquement (DaemonSet)
# - Si assez de restarts ‚Üí alerte KubePodCrashLooping
```

### 5.7 Cr√©er un Silence (ignorer une alerte)

1. https://alertmanager.ai-platform.localhost
2. Cliquer sur une alerte
3. **Silence** ‚Üí D√©finir dur√©e
4. L'alerte est masqu√©e pendant cette dur√©e

Via API :
```bash
# Cr√©er un silence pour TestAlert (2 heures)
curl -sk -X POST https://alertmanager.ai-platform.localhost/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [{"name": "alertname", "value": "TestAlert", "isRegex": false}],
    "startsAt": "'$(date -u +%Y-%m-%dT%H:%M:%S.000Z)'",
    "endsAt": "'$(date -u -d '+2 hours' +%Y-%m-%dT%H:%M:%S.000Z)'",
    "createdBy": "admin",
    "comment": "Test silence"
  }'
```

### 5.8 V√©rifier la configuration

```bash
# Status Alertmanager
curl -sk https://alertmanager.ai-platform.localhost/api/v2/status | jq .

# Configuration active
curl -sk https://alertmanager.ai-platform.localhost/api/v2/status | jq '.config.original'
```

### 5.9 Receivers (notifications) - Non configur√© en home lab

En production, on configurerait des receivers :

```yaml
# Exemple de configuration (values.yaml)
alertmanager:
  config:
    receivers:
      - name: 'slack'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/XXX'
            channel: '#alerts'
      - name: 'email'
        email_configs:
          - to: 'team@company.com'
    route:
      receiver: 'slack'
      group_by: ['alertname', 'namespace']
```

---

## D√©mo 6: Workflow complet - Incident simul√©

### Objectif
Simuler un incident et utiliser toute la stack pour diagnostiquer.

### Sc√©nario : Service RAG API down

```bash
# STEP 1: Provoquer l'incident
kubectl scale deployment -n ai-inference rag-api --replicas=0

# STEP 2: Observer dans Prometheus (2-5 min)
# https://prometheus.ai-platform.localhost/alerts
# ‚Üí KubeDeploymentReplicasMismatch passe en "Pending" puis "Firing"

# STEP 3: Observer dans Alertmanager
# https://alertmanager.ai-platform.localhost
# ‚Üí L'alerte appara√Æt

# STEP 4: Diagnostiquer dans Grafana
# Dashboards ‚Üí Kubernetes / Compute Resources / Namespace (Pods)
# S√©lectionner namespace: ai-inference
# ‚Üí Voir que rag-api n'a plus de pods

# STEP 5: Voir les logs dans Loki
# Explore ‚Üí Loki
# Query: {namespace="ai-inference", app="rag-api"}
# ‚Üí Voir les derniers logs avant l'arr√™t

# STEP 6: R√©soudre
kubectl scale deployment -n ai-inference rag-api --replicas=1

# STEP 7: V√©rifier la r√©solution
# Prometheus: alerte revient √† "Inactive"
# Alertmanager: alerte marqu√©e "Resolved"
# Grafana: pod r√©appara√Æt dans le dashboard
```

---

## D√©mo 7: Dashboard Kubernetes

### Objectif
Explorer les dashboards pr√©-install√©s.

### Dashboards recommand√©s

| Dashboard | Utilisation |
|-----------|-------------|
| Kubernetes / Compute Resources / Cluster | Vue globale |
| Kubernetes / Compute Resources / Namespace (Pods) | D√©tail par namespace |
| Kubernetes / Compute Resources / Node (Pods) | D√©tail par node |
| Node Exporter / Nodes | M√©triques syst√®me (CPU, RAM, Disk) |
| CoreDNS | DNS metrics |

### Navigation

1. **Dashboards** ‚Üí **Browse**
2. Filtrer par "Kubernetes" ou "Node"
3. S√©lectionner le namespace en haut (dropdown)

---

## D√©mo 8: Corr√©lation Logs/Metrics

### Objectif
Corr√©ler les m√©triques et logs pour diagnostiquer un probl√®me.

### Sc√©nario : Latence √©lev√©e sur RAG API

1. **Grafana** ‚Üí **Explore**
2. Split view (bouton "Split")
3. Gauche : **Prometheus** - `histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{namespace="ai-inference"}[5m]))`
4. Droite : **Loki** - `{namespace="ai-inference", app="rag-api"}`
5. Synchroniser le time range
6. Identifier les pics de latence et corr√©ler avec les logs

---

## D√©mo 9: Security Monitoring (Guardrails)

### Objectif
Visualiser les tentatives de prompt injection bloqu√©es.

### Query Loki - Blocked requests

```logql
{namespace="ai-apps", pod=~"open-webui-pipelines.*"} |= "Valid: false"
```

### Query Loki - All guardrails activity

```logql
{namespace="ai-apps"} |= "LLM Guard" | pattern "<_> User: <user>, Valid: <valid>, Risk: <risk>"
```

### Dashboard custom (√† cr√©er)

1. **Dashboards** ‚Üí **New** ‚Üí **New Dashboard**
2. **Add visualization**
3. Data source : Loki
4. Query : `count_over_time({namespace="ai-apps"} |= "Valid: false" [1h])`
5. Title : "Blocked Prompt Injections"

---

## R√©sum√© des URLs de test

| Service | URL | Ce qu'on peut v√©rifier |
|---------|-----|------------------------|
| **Prometheus** | https://prometheus.ai-platform.localhost | M√©triques, Alertes, Targets |
| **Prometheus /alerts** | https://prometheus.ai-platform.localhost/alerts | √âtat des r√®gles d'alertes |
| **Prometheus /targets** | https://prometheus.ai-platform.localhost/targets | Sources de m√©triques |
| **Alertmanager** | https://alertmanager.ai-platform.localhost | Alertes actives, Silences |
| **Grafana** | https://grafana.ai-platform.localhost | Dashboards, Explore |

---

## D√©mo 10: Falco - Runtime Security

### Objectif
D√©tecter les menaces runtime avec Falco.

### 10.1 V√©rifier le d√©ploiement

```bash
# Pods Falco (DaemonSet)
kubectl get pods -n falco

# Logs Falco
kubectl logs -n falco -l app.kubernetes.io/name=falco -f
```

### 10.2 Voir les alertes Falco dans Grafana/Loki

```logql
{namespace="falco"} | json | line_format "{{.priority}} {{.rule}} {{.output}}"
```

### 10.3 Test 1: Shell dans un container AI

```bash
# Ex√©cuter un shell dans un pod AI (d√©clenche une alerte)
kubectl exec -it -n ai-inference deploy/rag-api -- /bin/sh -c "whoami"

# V√©rifier les logs Falco
kubectl logs -n falco -l app.kubernetes.io/name=falco --tail=20 | grep -i shell
```

Alerte attendue:
```json
{
  "priority": "Notice",
  "rule": "Shell in AI Container",
  "output": "Shell spawned in AI container (user=root shell=sh container=rag-api namespace=ai-inference)"
}
```

### 10.4 Test 2: Acc√®s aux secrets

```bash
# Lire un fichier secret (d√©clenche une alerte)
kubectl exec -it -n ai-inference deploy/rag-api -- cat /var/run/secrets/kubernetes.io/serviceaccount/token

# V√©rifier
kubectl logs -n falco -l app.kubernetes.io/name=falco --tail=10 | grep -i secret
```

### 10.5 Test 3: Tentative d'exfiltration (simul√©e)

```bash
# Simuler une connexion externe depuis un pod AI
kubectl exec -it -n ai-inference deploy/rag-api -- curl -s https://example.com --max-time 5 || true

# V√©rifier les alertes r√©seau
kubectl logs -n falco -l app.kubernetes.io/name=falco --tail=10 | grep -i network
```

### 10.6 Falcosidekick UI

Si activ√©, acc√©der √† la UI:
```bash
kubectl port-forward -n falco svc/falco-falcosidekick-ui 2802:2802
# Ouvrir http://localhost:2802
```

---

## D√©mo 11: Kyverno - Policy Enforcement

### Objectif
Tester les politiques d'admission Kyverno.

### 11.1 V√©rifier le d√©ploiement

```bash
# Pods Kyverno
kubectl get pods -n kyverno

# Policies install√©es
kubectl get clusterpolicy

# D√©tails d'une policy
kubectl describe clusterpolicy require-resource-limits
```

### 11.2 Voir les Policy Reports

```bash
# Rapports par namespace
kubectl get policyreport -A

# D√©tails des violations
kubectl describe policyreport -n ai-inference
```

### 11.3 Test 1: Pod sans resource limits (VIOLATION)

```bash
# Cr√©er un pod sans limits
cat <<EOF | kubectl apply -f - --dry-run=server
apiVersion: v1
kind: Pod
metadata:
  name: test-no-limits
  namespace: ai-inference
spec:
  containers:
  - name: test
    image: nginx:latest
EOF

# R√©sultat (mode Audit): Warning affich√©
# R√©sultat (mode Enforce): Rejet√©
```

### 11.4 Test 2: Container privileged (BLOQU√â)

```bash
# Tenter de cr√©er un container privileged
cat <<EOF | kubectl apply -f - --dry-run=server
apiVersion: v1
kind: Pod
metadata:
  name: test-privileged
  namespace: ai-inference
spec:
  containers:
  - name: test
    image: nginx:1.25
    securityContext:
      privileged: true
EOF

# R√©sultat: Error - Privileged containers are not allowed
```

### 11.5 Test 3: Image avec tag :latest (VIOLATION)

```bash
# Cr√©er un pod avec :latest
cat <<EOF | kubectl apply -f - --dry-run=server
apiVersion: v1
kind: Pod
metadata:
  name: test-latest
  namespace: ai-inference
spec:
  containers:
  - name: test
    image: nginx:latest
    resources:
      limits:
        memory: "128Mi"
        cpu: "100m"
EOF

# R√©sultat (mode Audit): Warning - Using ':latest' tag is not allowed
```

### 11.6 Test 4: Pod conforme (ACCEPT√â)

```bash
# Cr√©er un pod conforme √† toutes les policies
cat <<EOF | kubectl apply -f - --dry-run=server
apiVersion: v1
kind: Pod
metadata:
  name: test-compliant
  namespace: ai-inference
spec:
  containers:
  - name: test
    image: nginx:1.25
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "100m"
    readinessProbe:
      httpGet:
        path: /
        port: 80
      periodSeconds: 10
EOF

# R√©sultat: Pod cr√©√© sans warnings ‚úÖ
```

### 11.7 Metrics Kyverno dans Prometheus

```promql
# Policies appliqu√©es
kyverno_policy_results_total

# Violations par policy
kyverno_policy_results_total{rule_result="fail"}

# Admissions bloqu√©es
kyverno_admission_requests_total{resource_request_operation="CREATE", success="false"}
```

---

## D√©mo 12: Cosign - Image Signature Verification

### Objectif
D√©montrer la v√©rification des signatures d'images.

### 12.1 Installer Cosign

```bash
# Linux
curl -sSL https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64 -o /usr/local/bin/cosign
chmod +x /usr/local/bin/cosign

# V√©rifier
cosign version
```

### 12.2 G√©n√©rer une paire de cl√©s

```bash
# Cr√©er r√©pertoire
mkdir -p ~/.cosign && cd ~/.cosign

# G√©n√©rer les cl√©s
cosign generate-key-pair

# Fichiers cr√©√©s:
# - cosign.key (priv√©e - GARDER SECR√àTE)
# - cosign.pub (publique - √† distribuer)
```

### 12.3 Signer une image

```bash
# Build et push une image de test
docker build -t ghcr.io/z3rox-lab/demo-app:v1.0.0 .
docker push ghcr.io/z3rox-lab/demo-app:v1.0.0

# Signer
cosign sign --key ~/.cosign/cosign.key ghcr.io/z3rox-lab/demo-app:v1.0.0

# V√©rifier
cosign verify --key ~/.cosign/cosign.pub ghcr.io/z3rox-lab/demo-app:v1.0.0
```

### 12.4 Voir l'arbre de signatures

```bash
cosign tree ghcr.io/z3rox-lab/demo-app:v1.0.0

# R√©sultat:
# üì¶ ghcr.io/z3rox-lab/demo-app:v1.0.0
# ‚îî‚îÄ‚îÄ üîê Signatures
#     ‚îî‚îÄ‚îÄ sha256:abc123...
```

### 12.5 Test Kyverno - Image non sign√©e (BLOQU√âE)

```bash
# D√©ployer une image non sign√©e (policy en Enforce)
kubectl run unsigned-app \
  --image=ghcr.io/z3rox-lab/unsigned-app:v1.0.0 \
  -n ai-inference

# R√©sultat attendu:
# Error: image signature verification failed for ghcr.io/z3rox-lab/unsigned-app:v1.0.0
```

### 12.6 Test Kyverno - Image sign√©e (ACCEPT√âE)

```bash
# D√©ployer une image sign√©e
kubectl run signed-app \
  --image=ghcr.io/z3rox-lab/demo-app:v1.0.0 \
  -n ai-inference

# R√©sultat: Pod cr√©√© ‚úÖ
```

### 12.7 Signature Keyless (OIDC)

```bash
# Signer avec authentification OIDC (ouvre navigateur)
COSIGN_EXPERIMENTAL=1 cosign sign ghcr.io/z3rox-lab/demo-app:v2.0.0

# V√©rifier avec identit√©
cosign verify \
  --certificate-identity "your-email@example.com" \
  --certificate-oidc-issuer "https://accounts.google.com" \
  ghcr.io/z3rox-lab/demo-app:v2.0.0
```

---

## Commandes utiles

### V√©rifier l'√©tat de la stack

```bash
# Pods
kubectl get pods -n observability

# PVCs
kubectl get pvc -n observability

# Services
kubectl get svc -n observability
```

### Logs des composants

```bash
# Prometheus
kubectl logs -n observability prometheus-kube-prometheus-stack-prometheus-0 -c prometheus

# Grafana
kubectl logs -n observability -l app.kubernetes.io/name=grafana

# Loki
kubectl logs -n observability loki-0

# Promtail
kubectl logs -n observability -l app.kubernetes.io/name=promtail
```

### M√©triques rapides

```bash
# CPU/Memory des pods
kubectl top pods -A --sort-by=memory | head -20

# Nodes
kubectl top nodes
```

### Test endpoints

```bash
# Prometheus
curl -sk https://prometheus.ai-platform.localhost/-/healthy

# Grafana
curl -sk https://grafana.ai-platform.localhost/api/health

# Alertmanager
curl -sk https://alertmanager.ai-platform.localhost/-/healthy

# Loki
curl -s http://localhost:3100/ready  # depuis un pod
```

---

## Script de d√©mo

```bash
#!/bin/bash
# demo-observability.sh

echo "=== Observability Demo ==="
echo ""

echo "1. Cluster Status"
kubectl top nodes
echo ""

echo "2. Pod Resources (top 10 by memory)"
kubectl top pods -A --sort-by=memory | head -11
echo ""

echo "3. Prometheus Targets"
curl -sk https://prometheus.ai-platform.localhost/api/v1/targets | jq '.data.activeTargets | length'
echo " active targets"
echo ""

echo "4. Loki Status"
kubectl exec -n observability loki-0 -- wget -qO- http://localhost:3100/ready
echo ""

echo "5. Recent Guardrails Activity (last 10)"
kubectl logs -n ai-apps deployment/open-webui-pipelines --tail=50 | grep "LLM Guard" | tail -10
echo ""

echo "=== Demo URLs ==="
echo "Grafana:      https://grafana.ai-platform.localhost"
echo "Prometheus:   https://prometheus.ai-platform.localhost"
echo "Alertmanager: https://alertmanager.ai-platform.localhost"
```

---

## Talking Points pour vid√©o

### Introduction (30s)
> "Voyons maintenant comment monitorer notre plateforme AI avec Prometheus et Grafana."

### M√©triques (1min)
> "Prometheus collecte les m√©triques de tous les composants. Ici on voit l'utilisation CPU et m√©moire par namespace. Notre LLM Ollama utilise environ 4GB de RAM."

### Logs (1min)
> "Loki centralise tous les logs. Je peux filtrer par namespace, application, ou chercher du texte. Ici je vois les logs du filtre LLM Guard qui bloque les prompt injections."

### Corr√©lation (30s)
> "Le split view permet de corr√©ler m√©triques et logs. Si je vois un pic de latence, je peux imm√©diatement voir les logs correspondants."

### Conclusion (30s)
> "Cette stack observability est l√©g√®re - environ 2GB de RAM - mais fournit une visibilit√© compl√®te sur la plateforme."

---

**Date:** 2026-02-03
**Author:** Z3ROX - AI Security Platform
**Version:** 1.0.0
