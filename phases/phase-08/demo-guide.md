# Phase 8: Observability - Demo Guide

## Overview

Ce guide fournit des exemples de requÃªtes et scÃ©narios de dÃ©monstration pour la stack observability.

## URLs

| Service | URL | Credentials |
|---------|-----|-------------|
| **Grafana** | https://grafana.ai-platform.localhost | admin / admin123! |
| **Prometheus** | https://prometheus.ai-platform.localhost | - |
| **Alertmanager** | https://alertmanager.ai-platform.localhost | - |

---

## DÃ©mo 1: MÃ©triques Cluster (Prometheus)

### Objectif
Visualiser les mÃ©triques du cluster Kubernetes.

### Via Grafana Dashboard

1. Ouvrir https://grafana.ai-platform.localhost
2. **Dashboards** â†’ Chercher "Kubernetes / Compute Resources / Cluster"
3. Observer : CPU, Memory, Network par namespace

### Via Prometheus directement

1. Ouvrir https://prometheus.ai-platform.localhost
2. **Graph** â†’ Entrer une requÃªte

### Exemples de requÃªtes PromQL

#### CPU Usage par namespace

```promql
sum(rate(container_cpu_usage_seconds_total{namespace!=""}[5m])) by (namespace)
```

#### Memory Usage par namespace

```promql
sum(container_memory_working_set_bytes{namespace!=""}) by (namespace) / 1024 / 1024 / 1024
```

#### Pods Running par namespace

```promql
count(kube_pod_status_phase{phase="Running"}) by (namespace)
```

#### CPU Usage des LLM components

```promql
sum(rate(container_cpu_usage_seconds_total{namespace=~"ai-inference|ai-apps"}[5m])) by (pod)
```

#### Memory des Guardrails

```promql
container_memory_working_set_bytes{namespace="ai-inference", pod=~"guardrails.*"} / 1024 / 1024
```

---

## DÃ©mo 2: Logs Application (Loki)

### Objectif
Explorer les logs des applications AI.

### Via Grafana Explore

1. Ouvrir https://grafana.ai-platform.localhost
2. **Explore** (icÃ´ne compas)
3. SÃ©lectionner **Loki** en haut
4. Entrer une requÃªte LogQL

### Exemples de requÃªtes LogQL

#### Logs Open WebUI

```logql
{namespace="ai-apps", app="open-webui"}
```

#### Logs Pipelines (Guardrails filter)

```logql
{namespace="ai-apps", pod=~"open-webui-pipelines.*"}
```

#### Logs LLM Guard avec filtrage

```logql
{namespace="ai-apps"} |= "LLM Guard"
```

#### Logs Guardrails API

```logql
{namespace="ai-inference", app="guardrails-api"}
```

#### Logs Ollama (LLM)

```logql
{namespace="ai-inference", app="ollama"}
```

#### Logs RAG API

```logql
{namespace="ai-inference", app="rag-api"}
```

#### Erreurs uniquement

```logql
{namespace="ai-inference"} |= "error" or |= "Error" or |= "ERROR"
```

#### Logs avec JSON parsing

```logql
{namespace="ai-inference", app="rag-api"} | json | line_format "{{.level}} {{.message}}"
```

#### Rate de logs par pod

```logql
sum(rate({namespace="ai-apps"}[5m])) by (pod)
```

---

## DÃ©mo 3: Monitoring LLM en temps rÃ©el

### Objectif
Surveiller les performances du LLM pendant une requÃªte.

### Setup

Terminal 1 - Watch logs :
```bash
kubectl logs -n ai-apps deployment/open-webui-pipelines -f | grep "LLM Guard"
```

Terminal 2 - Watch metrics :
```bash
watch kubectl top pods -n ai-inference
```

### Test

1. Ouvrir https://chat.ai-platform.localhost
2. Envoyer : "What is Kubernetes?"
3. Observer les logs et mÃ©triques

### Dans Grafana

1. **Explore** â†’ **Loki**
2. Query : `{namespace="ai-apps"} |= "LLM Guard"`
3. Cliquer **Live** (en haut Ã  droite)
4. Envoyer des messages dans le chat
5. Voir les logs en temps rÃ©el

---

## DÃ©mo 4: Prometheus - MÃ©triques & Alertes

### Objectif
Explorer Prometheus UI, tester des requÃªtes et comprendre les alertes.

### 4.1 Explorer Prometheus UI

1. Ouvrir https://prometheus.ai-platform.localhost
2. Les onglets principaux :
   - **Graph** : ExÃ©cuter des requÃªtes PromQL
   - **Alerts** : Voir les rÃ¨gles d'alertes et leur Ã©tat
   - **Status > Targets** : Voir ce que Prometheus scrape
   - **Status > Configuration** : Config active

### 4.2 VÃ©rifier les Targets

1. https://prometheus.ai-platform.localhost/targets
2. Tous les endpoints doivent Ãªtre **UP** (vert)
3. Si un target est **DOWN** (rouge) â†’ problÃ¨me de scraping

| Target | Description |
|--------|-------------|
| `kubernetes-apiservers` | API Kubernetes |
| `kubernetes-nodes` | MÃ©triques kubelet |
| `kubernetes-pods` | Pods avec annotations prometheus |
| `node-exporter` | MÃ©triques systÃ¨me (CPU, RAM, Disk) |
| `kube-state-metrics` | Ã‰tat des objets K8s |

### 4.3 Tester des requÃªtes PromQL

1. https://prometheus.ai-platform.localhost/graph
2. Entrer une requÃªte â†’ **Execute** â†’ Voir rÃ©sultat (Table ou Graph)

#### RequÃªtes de base

```promql
# Nombre de pods running par namespace
count(kube_pod_status_phase{phase="Running"}) by (namespace)

# Usage CPU total du cluster (%)
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Usage mÃ©moire par node (GB)
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / 1024 / 1024 / 1024

# Pods en restart (derniÃ¨re heure)
increase(kube_pod_container_status_restarts_total[1h]) > 0
```

#### RequÃªtes AI Platform spÃ©cifiques

```promql
# CPU des composants AI
sum(rate(container_cpu_usage_seconds_total{namespace=~"ai-inference|ai-apps"}[5m])) by (pod)

# MÃ©moire Ollama (MB)
container_memory_working_set_bytes{namespace="ai-inference", pod=~"ollama.*"} / 1024 / 1024

# MÃ©moire Guardrails (MB)
container_memory_working_set_bytes{namespace="ai-inference", pod=~"guardrails.*"} / 1024 / 1024

# Pods not ready
kube_pod_status_ready{condition="false"}
```

### 4.4 Comprendre les Ã©tats d'alertes

1. https://prometheus.ai-platform.localhost/alerts
2. Les 3 Ã©tats possibles :

| Ã‰tat | Couleur | Signification |
|------|---------|---------------|
| **Inactive** | ğŸŸ¢ Vert | Condition non remplie, tout va bien |
| **Pending** | ğŸŸ¡ Jaune | Condition remplie, attente de confirmation (for: duration) |
| **Firing** | ğŸ”´ Rouge | Alerte confirmÃ©e, envoyÃ©e Ã  Alertmanager |

### 4.5 Alertes prÃ©-configurÃ©es importantes

| Alerte | SÃ©vÃ©ritÃ© | Description |
|--------|----------|-------------|
| **Watchdog** | info | Toujours "Firing" - prouve que le systÃ¨me fonctionne |
| **KubePodCrashLooping** | warning | Pod restart en boucle |
| **KubePodNotReady** | warning | Pod non prÃªt > 15min |
| **KubeDeploymentReplicasMismatch** | warning | Replicas attendus â‰  rÃ©els |
| **NodeFilesystemSpaceFillingUp** | warning | Disque > 80% |
| **NodeMemoryHighUtilization** | warning | RAM > 90% |
| **PrometheusTargetMissing** | warning | Target de scraping down |

### 4.6 RequÃªtes pour voir les alertes

```promql
# Toutes les alertes qui "fire"
ALERTS{alertstate="firing"}

# Alertes pending
ALERTS{alertstate="pending"}

# Alertes par sÃ©vÃ©ritÃ©
ALERTS{severity="critical"}
ALERTS{severity="warning"}

# Alertes d'un namespace spÃ©cifique
ALERTS{namespace="ai-inference"}
```

---

## DÃ©mo 5: Alertmanager - Gestion des alertes

### Objectif
Comprendre Alertmanager et tester le systÃ¨me d'alertes.

### 5.1 Explorer Alertmanager UI

1. Ouvrir https://alertmanager.ai-platform.localhost
2. Les sections :
   - **Alerts** : Alertes actives reÃ§ues de Prometheus
   - **Silences** : Alertes mises en silence
   - **Status** : Configuration et Ã©tat

### 5.2 Architecture des alertes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Alerte      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Notification
â”‚ Prometheus â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Alertmanager â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
â”‚            â”‚                 â”‚              â”‚
â”‚ â€¢ DÃ©tecte  â”‚                 â”‚ â€¢ Groupe     â”‚    â€¢ Email
â”‚ â€¢ Ã‰value   â”‚                 â”‚ â€¢ DÃ©duplique â”‚    â€¢ Slack
â”‚ â€¢ Envoie   â”‚                 â”‚ â€¢ Route      â”‚    â€¢ PagerDuty
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â€¢ Webhook
```

### 5.3 VÃ©rifier les alertes actives

```bash
# Via curl (JSON)
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq .

# Alertes actives (noms seulement)
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq '.[].labels.alertname'

# Compter les alertes
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq 'length'
```

### 5.4 Test 1 : Envoyer une alerte manuelle

```bash
# CrÃ©er une alerte de test
curl -sk -X POST https://alertmanager.ai-platform.localhost/api/v2/alerts \
  -H "Content-Type: application/json" \
  -d '[{
    "labels": {
      "alertname": "TestAlert",
      "severity": "warning",
      "namespace": "demo",
      "service": "test-service"
    },
    "annotations": {
      "summary": "Ceci est une alerte de test",
      "description": "Test manuel de Alertmanager"
    },
    "generatorURL": "https://prometheus.ai-platform.localhost"
  }]'

# VÃ©rifier dans UI : https://alertmanager.ai-platform.localhost
# L'alerte "TestAlert" devrait apparaÃ®tre

# VÃ©rifier via API
curl -sk https://alertmanager.ai-platform.localhost/api/v2/alerts | jq '.[] | select(.labels.alertname=="TestAlert")'
```

### 5.5 Test 2 : DÃ©clencher une vraie alerte Kubernetes

```bash
# STEP 1: Scale down un deployment
kubectl scale deployment -n ai-apps open-webui-pipelines --replicas=0

echo "Attendre 2-5 minutes pour que l'alerte se dÃ©clenche..."
echo "Observer: https://prometheus.ai-platform.localhost/alerts"
echo "          â†’ KubeDeploymentReplicasMismatch devrait passer en Pending puis Firing"

# STEP 2: VÃ©rifier dans Prometheus (aprÃ¨s 2 min)
# https://prometheus.ai-platform.localhost/alerts
# Chercher: KubeDeploymentReplicasMismatch â†’ Ã©tat "Pending" ou "Firing"

# STEP 3: VÃ©rifier dans Alertmanager (aprÃ¨s 5 min)
# https://alertmanager.ai-platform.localhost
# L'alerte devrait apparaÃ®tre

# STEP 4: RESTAURER !
kubectl scale deployment -n ai-apps open-webui-pipelines --replicas=1

# STEP 5: Observer l'alerte disparaÃ®tre
# Dans Prometheus: Ã©tat revient Ã  "Inactive"
# Dans Alertmanager: alerte rÃ©solue
```

### 5.6 Test 3 : Simuler un pod crash

```bash
# Tuer un pod pour simuler un crash
kubectl delete pod -n ai-apps -l app.kubernetes.io/name=promtail --force

# Observer:
# - Le pod restart automatiquement (DaemonSet)
# - Si assez de restarts â†’ alerte KubePodCrashLooping
```

### 5.7 CrÃ©er un Silence (ignorer une alerte)

1. https://alertmanager.ai-platform.localhost
2. Cliquer sur une alerte
3. **Silence** â†’ DÃ©finir durÃ©e
4. L'alerte est masquÃ©e pendant cette durÃ©e

Via API :
```bash
# CrÃ©er un silence pour TestAlert (2 heures)
curl -sk -X POST https://alertmanager.ai-platform.localhost/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [{"name": "alertname", "value": "TestAlert", "isRegex": false}],
    "startsAt": "'$(date -u +%Y-%m-%dT%H:%M:%S.000Z)'",
    "endsAt": "'$(date -u -d '+2 hours' +%Y-%m-%dT%H:%M:%S.000Z)'",
    "createdBy": "admin",
    "comment": "Test silence"
  }'
```

### 5.8 VÃ©rifier la configuration

```bash
# Status Alertmanager
curl -sk https://alertmanager.ai-platform.localhost/api/v2/status | jq .

# Configuration active
curl -sk https://alertmanager.ai-platform.localhost/api/v2/status | jq '.config.original'
```

### 5.9 Receivers (notifications) - Non configurÃ© en home lab

En production, on configurerait des receivers :

```yaml
# Exemple de configuration (values.yaml)
alertmanager:
  config:
    receivers:
      - name: 'slack'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/XXX'
            channel: '#alerts'
      - name: 'email'
        email_configs:
          - to: 'team@company.com'
    route:
      receiver: 'slack'
      group_by: ['alertname', 'namespace']
```

---

## DÃ©mo 6: Workflow complet - Incident simulÃ©

### Objectif
Simuler un incident et utiliser toute la stack pour diagnostiquer.

### ScÃ©nario : Service RAG API down

```bash
# STEP 1: Provoquer l'incident
kubectl scale deployment -n ai-inference rag-api --replicas=0

# STEP 2: Observer dans Prometheus (2-5 min)
# https://prometheus.ai-platform.localhost/alerts
# â†’ KubeDeploymentReplicasMismatch passe en "Pending" puis "Firing"

# STEP 3: Observer dans Alertmanager
# https://alertmanager.ai-platform.localhost
# â†’ L'alerte apparaÃ®t

# STEP 4: Diagnostiquer dans Grafana
# Dashboards â†’ Kubernetes / Compute Resources / Namespace (Pods)
# SÃ©lectionner namespace: ai-inference
# â†’ Voir que rag-api n'a plus de pods

# STEP 5: Voir les logs dans Loki
# Explore â†’ Loki
# Query: {namespace="ai-inference", app="rag-api"}
# â†’ Voir les derniers logs avant l'arrÃªt

# STEP 6: RÃ©soudre
kubectl scale deployment -n ai-inference rag-api --replicas=1

# STEP 7: VÃ©rifier la rÃ©solution
# Prometheus: alerte revient Ã  "Inactive"
# Alertmanager: alerte marquÃ©e "Resolved"
# Grafana: pod rÃ©apparaÃ®t dans le dashboard
```

---

## DÃ©mo 7: Dashboard Kubernetes

### Objectif
Explorer les dashboards prÃ©-installÃ©s.

### Dashboards recommandÃ©s

| Dashboard | Utilisation |
|-----------|-------------|
| Kubernetes / Compute Resources / Cluster | Vue globale |
| Kubernetes / Compute Resources / Namespace (Pods) | DÃ©tail par namespace |
| Kubernetes / Compute Resources / Node (Pods) | DÃ©tail par node |
| Node Exporter / Nodes | MÃ©triques systÃ¨me (CPU, RAM, Disk) |
| CoreDNS | DNS metrics |

### Navigation

1. **Dashboards** â†’ **Browse**
2. Filtrer par "Kubernetes" ou "Node"
3. SÃ©lectionner le namespace en haut (dropdown)

---

## DÃ©mo 8: CorrÃ©lation Logs/Metrics

### Objectif
CorrÃ©ler les mÃ©triques et logs pour diagnostiquer un problÃ¨me.

### ScÃ©nario : Latence Ã©levÃ©e sur RAG API

1. **Grafana** â†’ **Explore**
2. Split view (bouton "Split")
3. Gauche : **Prometheus** - `histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{namespace="ai-inference"}[5m]))`
4. Droite : **Loki** - `{namespace="ai-inference", app="rag-api"}`
5. Synchroniser le time range
6. Identifier les pics de latence et corrÃ©ler avec les logs

---

## DÃ©mo 9: Security Monitoring (Guardrails)

### Objectif
Visualiser les tentatives de prompt injection bloquÃ©es.

### Query Loki - Blocked requests

```logql
{namespace="ai-apps", pod=~"open-webui-pipelines.*"} |= "Valid: false"
```

### Query Loki - All guardrails activity

```logql
{namespace="ai-apps"} |= "LLM Guard" | pattern "<_> User: <user>, Valid: <valid>, Risk: <risk>"
```

### Dashboard custom (Ã  crÃ©er)

1. **Dashboards** â†’ **New** â†’ **New Dashboard**
2. **Add visualization**
3. Data source : Loki
4. Query : `count_over_time({namespace="ai-apps"} |= "Valid: false" [1h])`
5. Title : "Blocked Prompt Injections"

---

## RÃ©sumÃ© des URLs de test

| Service | URL | Ce qu'on peut vÃ©rifier |
|---------|-----|------------------------|
| **Prometheus** | https://prometheus.ai-platform.localhost | MÃ©triques, Alertes, Targets |
| **Prometheus /alerts** | https://prometheus.ai-platform.localhost/alerts | Ã‰tat des rÃ¨gles d'alertes |
| **Prometheus /targets** | https://prometheus.ai-platform.localhost/targets | Sources de mÃ©triques |
| **Alertmanager** | https://alertmanager.ai-platform.localhost | Alertes actives, Silences |
| **Grafana** | https://grafana.ai-platform.localhost | Dashboards, Explore |

---

## Commandes utiles

### VÃ©rifier l'Ã©tat de la stack

```bash
# Pods
kubectl get pods -n observability

# PVCs
kubectl get pvc -n observability

# Services
kubectl get svc -n observability
```

### Logs des composants

```bash
# Prometheus
kubectl logs -n observability prometheus-kube-prometheus-stack-prometheus-0 -c prometheus

# Grafana
kubectl logs -n observability -l app.kubernetes.io/name=grafana

# Loki
kubectl logs -n observability loki-0

# Promtail
kubectl logs -n observability -l app.kubernetes.io/name=promtail
```

### MÃ©triques rapides

```bash
# CPU/Memory des pods
kubectl top pods -A --sort-by=memory | head -20

# Nodes
kubectl top nodes
```

### Test endpoints

```bash
# Prometheus
curl -sk https://prometheus.ai-platform.localhost/-/healthy

# Grafana
curl -sk https://grafana.ai-platform.localhost/api/health

# Alertmanager
curl -sk https://alertmanager.ai-platform.localhost/-/healthy

# Loki
curl -s http://localhost:3100/ready  # depuis un pod
```

---

## Script de dÃ©mo

```bash
#!/bin/bash
# demo-observability.sh

echo "=== Observability Demo ==="
echo ""

echo "1. Cluster Status"
kubectl top nodes
echo ""

echo "2. Pod Resources (top 10 by memory)"
kubectl top pods -A --sort-by=memory | head -11
echo ""

echo "3. Prometheus Targets"
curl -sk https://prometheus.ai-platform.localhost/api/v1/targets | jq '.data.activeTargets | length'
echo " active targets"
echo ""

echo "4. Loki Status"
kubectl exec -n observability loki-0 -- wget -qO- http://localhost:3100/ready
echo ""

echo "5. Recent Guardrails Activity (last 10)"
kubectl logs -n ai-apps deployment/open-webui-pipelines --tail=50 | grep "LLM Guard" | tail -10
echo ""

echo "=== Demo URLs ==="
echo "Grafana:      https://grafana.ai-platform.localhost"
echo "Prometheus:   https://prometheus.ai-platform.localhost"
echo "Alertmanager: https://alertmanager.ai-platform.localhost"
```

---

## Talking Points pour vidÃ©o

### Introduction (30s)
> "Voyons maintenant comment monitorer notre plateforme AI avec Prometheus et Grafana."

### MÃ©triques (1min)
> "Prometheus collecte les mÃ©triques de tous les composants. Ici on voit l'utilisation CPU et mÃ©moire par namespace. Notre LLM Ollama utilise environ 4GB de RAM."

### Logs (1min)
> "Loki centralise tous les logs. Je peux filtrer par namespace, application, ou chercher du texte. Ici je vois les logs du filtre LLM Guard qui bloque les prompt injections."

### CorrÃ©lation (30s)
> "Le split view permet de corrÃ©ler mÃ©triques et logs. Si je vois un pic de latence, je peux immÃ©diatement voir les logs correspondants."

### Conclusion (30s)
> "Cette stack observability est lÃ©gÃ¨re - environ 2GB de RAM - mais fournit une visibilitÃ© complÃ¨te sur la plateforme."

---

**Date:** 2026-02-03
**Author:** Z3ROX - AI Security Platform
**Version:** 1.0.0
