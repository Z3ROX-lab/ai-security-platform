# Phase 5: Ollama LLM Inference

## Overview

Phase 5 deploys a local Large Language Model (LLM) inference server using Ollama with Mistral 7B. This enables the AI Security Platform to generate AI responses without sending data to external APIs.

| Aspect | Details |
|--------|---------|
| **Runtime** | Ollama |
| **Model** | Mistral 7B Instruct Q4_K_M |
| **RAM Usage** | ~5-6 GB |
| **Inference Speed** | 5-15 tokens/second (CPU) |
| **API** | OpenAI-compatible |

---

# Part 1: Understanding LLM Inference

## What is LLM Inference?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LLM INFERENCE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  INPUT (Prompt)                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ "What is Kubernetes in one sentence?"                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      LLM (Mistral 7B)                            â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  â€¢ 7 billion parameters                                          â”‚    â”‚
â”‚  â”‚  â€¢ Transformer architecture                                       â”‚    â”‚
â”‚  â”‚  â€¢ Generates tokens one by one                                   â”‚    â”‚
â”‚  â”‚  â€¢ Each token ~50-200ms on CPU                                   â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  OUTPUT (Generated Text)                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ "Kubernetes is an open-source platform designed for automating   â”‚    â”‚
â”‚  â”‚  the deployment, scaling, and management of containerized        â”‚    â”‚
â”‚  â”‚  applications."                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Why Local Inference?

| Aspect | Cloud API (OpenAI) | Local (Ollama) |
|--------|-------------------|----------------|
| **Data Privacy** | âŒ Data sent externally | âœ… Data stays local |
| **Cost** | ğŸ’° Per-token pricing | âœ… Free (hardware cost only) |
| **Latency** | ~500ms network overhead | âœ… Direct inference |
| **Availability** | Depends on provider | âœ… Always available |
| **Customization** | Limited | âœ… Full control |
| **Quality** | â­â­â­â­â­ GPT-4 | â­â­â­ Good for most tasks |

---

# Part 2: Ollama Architecture

## What is Ollama?

Ollama is a user-friendly LLM runner that simplifies model management and inference. It wraps llama.cpp (highly optimized C++ inference engine) with an easy-to-use CLI and API.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         OLLAMA ARCHITECTURE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      OLLAMA SERVER                               â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚  REST API     â”‚    â”‚  Model        â”‚    â”‚  llama.cpp    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚               â”‚    â”‚  Manager      â”‚    â”‚  Engine       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  Port: 11434  â”‚    â”‚               â”‚    â”‚               â”‚    â”‚    â”‚
â”‚  â”‚  â”‚               â”‚    â”‚  â€¢ Pull       â”‚    â”‚  â€¢ Quantized  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  Endpoints:   â”‚    â”‚  â€¢ List       â”‚    â”‚    inference  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ /api/gen   â”‚    â”‚  â€¢ Delete     â”‚    â”‚  â€¢ CPU/GPU    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ /api/chat  â”‚    â”‚  â€¢ Copy       â”‚    â”‚    optimized  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â€¢ /v1/...    â”‚    â”‚               â”‚    â”‚               â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    MODEL STORAGE                                 â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  /root/.ollama/models/                                           â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ manifests/                                                  â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€ registry.ollama.ai/                                    â”‚    â”‚
â”‚  â”‚  â”‚       â””â”€â”€ library/                                           â”‚    â”‚
â”‚  â”‚  â”‚           â””â”€â”€ mistral/                                       â”‚    â”‚
â”‚  â”‚  â””â”€â”€ blobs/                                                      â”‚    â”‚
â”‚  â”‚      â””â”€â”€ sha256-xxxxx  (4.4 GB model weights)                   â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  PVC: 20Gi (Kubernetes Persistent Volume)                        â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ollama vs Alternatives

| Feature | Ollama | vLLM | llama.cpp | LocalAI |
|---------|--------|------|-----------|---------|
| **Ease of Use** | â­â­â­ | â­ | â­â­ | â­â­ |
| **CPU Performance** | â­â­â­ | â­ | â­â­â­ | â­â­â­ |
| **GPU Performance** | â­â­ | â­â­â­ | â­â­â­ | â­â­ |
| **Model Management** | â­â­â­ | â­â­ | â­ | â­â­ |
| **OpenAI API** | âœ… | âœ… | âœ… | âœ… |
| **K8s Ready** | âœ… | âœ… | âš ï¸ | âœ… |

**Why Ollama?** Best balance of simplicity and performance for CPU-only home lab.

---

# Part 3: Model Selection

## Understanding Model Sizes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL SIZE vs QUALITY vs SPEED                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Parameters    RAM Needed     Quality        Speed (CPU)                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚                                                                          â”‚
â”‚    3B          ~2 GB          â­â­           â­â­â­ Fast                  â”‚
â”‚    â”‚                          (Basic)        (~20 tok/s)                â”‚
â”‚    â”‚                                                                     â”‚
â”‚    7B          ~5 GB          â­â­â­         â­â­â­ Good                  â”‚
â”‚    â”‚                          (Good)         (~10 tok/s)                â”‚
â”‚    â”‚                                                                     â”‚
â”‚   13B          ~10 GB         â­â­â­â­       â­â­ Slower                  â”‚
â”‚    â”‚                          (Better)       (~5 tok/s)                 â”‚
â”‚    â”‚                                                                     â”‚
â”‚   70B          ~40 GB         â­â­â­â­â­     â­ Very Slow                â”‚
â”‚                               (Best)         (~1 tok/s)                 â”‚
â”‚                                                                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                          â”‚
â”‚  Our Choice: 7B (Mistral)                                               â”‚
â”‚  â€¢ Best quality/size ratio                                              â”‚
â”‚  â€¢ Fits in 32GB system with other services                             â”‚
â”‚  â€¢ Good speed for interactive use                                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quantization Explained

Models are compressed using **quantization** to reduce memory usage:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        QUANTIZATION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  ORIGINAL MODEL (FP16)                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Each weight: 16 bits (2 bytes)                                  â”‚    â”‚
â”‚  â”‚  Mistral 7B: 7 billion Ã— 2 bytes = ~14 GB                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â”‚ Quantization                              â”‚
â”‚                              â–¼                                           â”‚
â”‚  QUANTIZED MODEL (Q4_K_M)                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Each weight: ~4.5 bits (0.56 bytes)                            â”‚    â”‚
â”‚  â”‚  Mistral 7B: 7 billion Ã— 0.56 bytes = ~4 GB                     â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  Quality loss: < 1% (barely noticeable)                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚  QUANTIZATION LEVELS                                                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
â”‚                                                                          â”‚
â”‚  Q2_K   â†’ 2.5 bits â†’ ~2.5 GB â†’ âŒ Significant quality loss             â”‚
â”‚  Q3_K_M â†’ 3.4 bits â†’ ~3.0 GB â†’ âš ï¸ Noticeable quality loss              â”‚
â”‚  Q4_K_M â†’ 4.5 bits â†’ ~4.0 GB â†’ âœ… Best balance (our choice)            â”‚
â”‚  Q5_K_M â†’ 5.5 bits â†’ ~5.0 GB â†’ âœ… Higher quality                        â”‚
â”‚  Q6_K   â†’ 6.5 bits â†’ ~6.0 GB â†’ âœ… Near-original quality                â”‚
â”‚  Q8_0   â†’ 8.0 bits â†’ ~7.5 GB â†’ âœ… Minimal loss                         â”‚
â”‚  FP16   â†’ 16 bits  â†’ ~14 GB  â†’ Original (too heavy)                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Why Mistral 7B?

| Model | Strengths | Weaknesses |
|-------|-----------|------------|
| **Mistral 7B** âœ… | Excellent instruction following, coding, reasoning | English-focused |
| Llama 3.1 8B | Good all-around, multilingual | Slightly larger |
| Phi-3 Mini (3.8B) | Very fast, efficient | Lower quality for complex tasks |
| Qwen2.5 7B | Good for Chinese/multilingual | Newer, less tested |

**Decision**: Mistral 7B offers the best quality for its size, especially for technical content (Kubernetes, security, DevOps).

---

# Part 4: Kubernetes Deployment

## Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OLLAMA IN KUBERNETES                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Namespace: ai-inference                                                â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                    â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      DEPLOYMENT                                  â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚                    POD                                   â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚           CONTAINER: ollama                     â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚                                                 â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  Image: ollama/ollama:latest                   â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  Port: 11434                                   â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚                                                 â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  Resources:                                     â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    requests: 6Gi RAM, 2 CPU                    â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    limits:   8Gi RAM, 4 CPU                    â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚                                                 â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  Environment:                                   â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    OLLAMA_KEEP_ALIVE: 24h                      â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    OLLAMA_NUM_PARALLEL: 1                      â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚                                                 â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                         â”‚                                â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                         â”‚ Volume Mount                   â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                         â–¼                                â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚              PVC: ollama-data                   â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚              Size: 20Gi                         â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚              Path: /root/.ollama               â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚                                                 â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  Contains:                                      â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  â€¢ Model weights (mistral 4.4GB)               â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  â€¢ Model manifests                              â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  â€¢ Cache                                        â”‚     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                                                          â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      SERVICE                                       â”‚   â”‚
â”‚  â”‚                                                                    â”‚   â”‚
â”‚  â”‚  Name: ollama                                                      â”‚   â”‚
â”‚  â”‚  Type: ClusterIP                                                   â”‚   â”‚
â”‚  â”‚  Port: 11434                                                       â”‚   â”‚
â”‚  â”‚                                                                    â”‚   â”‚
â”‚  â”‚  DNS: ollama.ai-inference.svc.cluster.local                       â”‚   â”‚
â”‚  â”‚                                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ArgoCD Application Structure

```
argocd/applications/ai/ollama/
â”œâ”€â”€ application.yaml          # ArgoCD Application definition
â””â”€â”€ values.yaml               # Helm chart configuration
```

### application.yaml

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ollama
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  sources:
    # Helm chart from otwld repository
    - repoURL: https://otwld.github.io/ollama-helm
      chart: ollama
      targetRevision: 0.52.0
      helm:
        valueFiles:
          - $values/argocd/applications/ai/ollama/values.yaml
    # Our values from Git
    - repoURL: https://github.com/Z3ROX-lab/ai-security-platform
      targetRevision: master
      ref: values
  destination:
    server: https://kubernetes.default.svc
    namespace: ai-inference
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

### values.yaml

```yaml
# Ollama values for AI Security Platform
# Based on ADR-008: LLM Inference Strategy

# Pre-pull models on startup
ollama:
  models:
    - mistral:7b-instruct-v0.3-q4_K_M

  # GPU disabled (CPU only - home lab)
  gpu:
    enabled: false

# Resources (Mistral 7B Q4_K_M needs ~5GB)
resources:
  requests:
    memory: "6Gi"
    cpu: "2000m"
  limits:
    memory: "8Gi"
    cpu: "4000m"

# Persistent storage for models
persistentVolume:
  enabled: true
  size: 20Gi
  storageClass: local-path

# Service configuration
service:
  type: ClusterIP
  port: 11434

# Keep model loaded (avoid cold start)
extraEnv:
  - name: OLLAMA_KEEP_ALIVE
    value: "24h"
  - name: OLLAMA_NUM_PARALLEL
    value: "1"

# Probes (model loading can be slow)
livenessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 120
  periodSeconds: 30
  timeoutSeconds: 5

readinessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
```

---

# Part 5: API Reference

## Ollama API Endpoints

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         OLLAMA API                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Base URL: http://ollama.ai-inference.svc:11434                         â”‚
â”‚                                                                          â”‚
â”‚  NATIVE OLLAMA API                                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                       â”‚
â”‚                                                                          â”‚
â”‚  POST /api/generate     Generate text (single prompt)                   â”‚
â”‚  POST /api/chat         Chat with message history                       â”‚
â”‚  POST /api/embeddings   Generate embeddings                             â”‚
â”‚  POST /api/pull         Download a model                                â”‚
â”‚  GET  /api/tags         List available models                           â”‚
â”‚  DELETE /api/delete     Remove a model                                  â”‚
â”‚  POST /api/copy         Copy a model                                    â”‚
â”‚  POST /api/create       Create custom model (Modelfile)                 â”‚
â”‚  GET  /api/ps           List running models                             â”‚
â”‚                                                                          â”‚
â”‚  OPENAI-COMPATIBLE API                                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                   â”‚
â”‚                                                                          â”‚
â”‚  POST /v1/chat/completions    OpenAI chat format                        â”‚
â”‚  POST /v1/completions         OpenAI completion format                  â”‚
â”‚  GET  /v1/models              List models (OpenAI format)               â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Examples

### Generate (Native API)

```bash
curl http://ollama.ai-inference.svc:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral:7b-instruct-v0.3-q4_K_M",
    "prompt": "What is a Kubernetes Pod?",
    "stream": false
  }'
```

Response:
```json
{
  "model": "mistral:7b-instruct-v0.3-q4_K_M",
  "response": "A Kubernetes Pod is the smallest deployable unit...",
  "done": true,
  "total_duration": 5234567890,
  "eval_count": 45
}
```

### Chat (Native API)

```bash
curl http://ollama.ai-inference.svc:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral:7b-instruct-v0.3-q4_K_M",
    "messages": [
      {"role": "system", "content": "You are a Kubernetes expert."},
      {"role": "user", "content": "How do I create a NetworkPolicy?"}
    ],
    "stream": false
  }'
```

### OpenAI-Compatible (for integrations)

```bash
curl http://ollama.ai-inference.svc:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral:7b-instruct-v0.3-q4_K_M",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Explain RBAC in Kubernetes"}
    ]
  }'
```

### Model Management

```bash
# List models
curl http://ollama.ai-inference.svc:11434/api/tags

# Pull a new model
curl http://ollama.ai-inference.svc:11434/api/pull \
  -d '{"name": "phi3:mini"}'

# Delete a model
curl -X DELETE http://ollama.ai-inference.svc:11434/api/delete \
  -d '{"name": "old-model"}'
```

---

# Part 6: Integration with RAG Pipeline

## How Ollama Fits in the RAG Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE INTEGRATION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  USER QUERY                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ "What does our security policy say about network segmentation?" â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  EMBEDDING SERVICE (Phase 6)                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Convert query to vector [0.23, -0.45, 0.12, ...]               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  QDRANT VECTOR DB (Phase 6)                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Find similar documents:                                         â”‚    â”‚
â”‚  â”‚  â€¢ Chunk 1: "Network segmentation policy requires..."           â”‚    â”‚
â”‚  â”‚  â€¢ Chunk 2: "All inter-namespace traffic must..."               â”‚    â”‚
â”‚  â”‚  â€¢ Chunk 3: "Production networks are isolated..."               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  CONTEXT ASSEMBLY                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  System: You are a security assistant. Answer based on context. â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  Context:                                                         â”‚    â”‚
â”‚  â”‚  [Chunk 1: Network segmentation policy requires...]             â”‚    â”‚
â”‚  â”‚  [Chunk 2: All inter-namespace traffic must...]                 â”‚    â”‚
â”‚  â”‚  [Chunk 3: Production networks are isolated...]                 â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  User: What does our security policy say about network          â”‚    â”‚
â”‚  â”‚        segmentation?                                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         OLLAMA                                   â”‚    â”‚
â”‚  â”‚                      (Mistral 7B)                                â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  Generates response using:                                       â”‚    â”‚
â”‚  â”‚  â€¢ Retrieved context (RAG)                                       â”‚    â”‚
â”‚  â”‚  â€¢ Model knowledge                                               â”‚    â”‚
â”‚  â”‚  â€¢ Instruction following                                         â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  RESPONSE                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ "According to our security policy, network segmentation         â”‚    â”‚
â”‚  â”‚  requires:                                                       â”‚    â”‚
â”‚  â”‚  1. All inter-namespace traffic must use NetworkPolicies        â”‚    â”‚
â”‚  â”‚  2. Production networks must be isolated from development       â”‚    â”‚
â”‚  â”‚  3. Default deny policies must be applied to all namespaces     â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  [Sources: security-policy.md, network-standards.md]"           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 7: Performance & Optimization

## Expected Performance (CPU Only)

| Metric | Value | Notes |
|--------|-------|-------|
| **First token latency** | 2-5 seconds | Model loading if cold |
| **Tokens per second** | 5-15 tok/s | Depends on CPU |
| **Memory (model loaded)** | ~5 GB | Plus ~1GB overhead |
| **Context processing** | ~1-2 tok/s | For long prompts |

## Optimization Settings

```yaml
# Environment variables for optimization
extraEnv:
  # Keep model loaded in memory (avoid cold start)
  - name: OLLAMA_KEEP_ALIVE
    value: "24h"
  
  # Number of parallel requests (1 for home lab)
  - name: OLLAMA_NUM_PARALLEL
    value: "1"
  
  # Number of CPU threads (match your CPU cores)
  - name: OLLAMA_NUM_THREADS
    value: "4"
  
  # Flash attention (faster on supported CPUs)
  - name: OLLAMA_FLASH_ATTENTION
    value: "1"
```

## Memory Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY LIFECYCLE                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  COLD START (first request)                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. Ollama receives request                                      â”‚    â”‚
â”‚  â”‚  2. Loads model from disk to RAM (~5GB, takes 10-30 seconds)    â”‚    â”‚
â”‚  â”‚  3. Processes request                                            â”‚    â”‚
â”‚  â”‚  4. Returns response                                             â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  First token latency: 10-30 seconds                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚  WARM (model already loaded)                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. Ollama receives request                                      â”‚    â”‚
â”‚  â”‚  2. Model already in RAM                                         â”‚    â”‚
â”‚  â”‚  3. Processes request immediately                                â”‚    â”‚
â”‚  â”‚  4. Returns response                                             â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  First token latency: 2-5 seconds                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”‚  OLLAMA_KEEP_ALIVE=24h                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Model stays loaded for 24 hours after last request             â”‚    â”‚
â”‚  â”‚  â†’ Always warm for active use                                   â”‚    â”‚
â”‚  â”‚  â†’ Unloads after 24h of inactivity to free RAM                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 8: Security Considerations

## OWASP LLM Top 10 Coverage

| Risk | Mitigation | Status |
|------|------------|--------|
| **LLM01: Prompt Injection** | Guardrails (Phase 7) | ğŸ”² Planned |
| **LLM02: Insecure Output** | Output validation (Phase 7) | ğŸ”² Planned |
| **LLM03: Training Data Poisoning** | Use trusted models (Ollama registry) | âœ… Done |
| **LLM04: Model DoS** | Resource limits in K8s | âœ… Done |
| **LLM05: Supply Chain** | Pinned chart version (0.52.0) | âœ… Done |
| **LLM06: Sensitive Info** | No PII in prompts, guardrails | ğŸ”² Planned |
| **LLM07: Insecure Plugin** | No plugins in Ollama | âœ… N/A |
| **LLM08: Excessive Agency** | RAG only, no tool use | âœ… Done |
| **LLM09: Overreliance** | Disclaimers (Phase 7) | ğŸ”² Planned |
| **LLM10: Model Theft** | NetworkPolicies | ğŸ”² Planned |

## NetworkPolicy (to add in Phase 4)

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ollama-policy
  namespace: ai-inference
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: ollama
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow from RAG API / Open WebUI
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ai-apps
      ports:
        - protocol: TCP
          port: 11434
    # Allow from guardrails (for NeMo LLM checks)
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ai-security
      ports:
        - protocol: TCP
          port: 11434
  egress:
    # DNS only (no internet needed after model download)
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
      ports:
        - protocol: UDP
          port: 53
```

---

# Part 9: Troubleshooting

## Common Issues

### Model Not Loading

```bash
# Check pod status
kubectl get pods -n ai-inference

# Check logs
kubectl logs -n ai-inference -l app.kubernetes.io/name=ollama

# Check if model exists
kubectl exec -n ai-inference <pod-name> -- ollama list
```

### Out of Memory

```bash
# Check memory usage
kubectl top pod -n ai-inference

# Solution: Use smaller model or increase limits
kubectl edit deployment ollama -n ai-inference
```

### Slow Response

```bash
# Check if model is loaded (warm)
kubectl exec -n ai-inference <pod-name> -- ollama ps

# If empty, model needs to load first
# Send a simple request to warm up
kubectl exec -n ai-inference <pod-name> -- ollama run mistral "Hi"
```

### PVC Issues

```bash
# Check PVC status
kubectl get pvc -n ai-inference

# Check storage
kubectl exec -n ai-inference <pod-name> -- df -h /root/.ollama
```

## Useful Commands

```bash
# List models
kubectl exec -n ai-inference <pod-name> -- ollama list

# Show running models
kubectl exec -n ai-inference <pod-name> -- ollama ps

# Pull new model
kubectl exec -n ai-inference <pod-name> -- ollama pull phi3:mini

# Delete model
kubectl exec -n ai-inference <pod-name> -- ollama rm old-model

# Interactive test
kubectl exec -it -n ai-inference <pod-name> -- ollama run mistral

# Check API
kubectl exec -n ai-inference <pod-name> -- curl localhost:11434/api/tags
```

---

# Part 10: Adding More Models

## Multi-Model Strategy

```yaml
# values.yaml - multiple models
ollama:
  models:
    - mistral:7b-instruct-v0.3-q4_K_M   # Primary (quality)
    - phi3:mini                          # Secondary (fast)
```

## Model Selection Logic (Future)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL ROUTING                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  Query Analysis                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  Simple Q&A         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  phi3:mini (fast)       â”‚    â”‚
â”‚  â”‚  "What is X?"                             ~2.5GB, ~20 tok/s      â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  Complex/Technical  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  mistral:7b (quality)   â”‚    â”‚
â”‚  â”‚  "How to implement..."                    ~5GB, ~10 tok/s        â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â”‚  Code Generation    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  mistral:7b (quality)   â”‚    â”‚
â”‚  â”‚  "Write a function..."                                           â”‚    â”‚
â”‚  â”‚                                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Summary

## What We Deployed

| Component | Details |
|-----------|---------|
| **Ollama Server** | v0.52.0 (Helm chart) |
| **Model** | Mistral 7B Instruct Q4_K_M (4.4 GB) |
| **Namespace** | ai-inference |
| **RAM** | 6-8 GB |
| **Storage** | 20 GB PVC |
| **API** | Port 11434, OpenAI-compatible |

## Architecture Position

```
Phase 1 â”€â”€â–¶ Phase 2 â”€â”€â–¶ Phase 3 â”€â”€â–¶ Phase 4 â”€â”€â–¶ Phase 5 â”€â”€â–¶ Phase 6
K3d/ArgoCD   PostgreSQL   Keycloak    Security    OLLAMA     Qdrant
                          Traefik     Baseline    âœ… DONE    (RAG)
```

## Next Steps

| Phase | Component | Dependency |
|-------|-----------|------------|
| **6** | Qdrant + Embedding | Phase 5 âœ… |
| **7** | Guardrails | Phase 5 âœ…, Phase 6 |
| **10** | Open WebUI | Phase 3, Phase 5 âœ… |

## Quick Reference

```bash
# Check status
kubectl get pods -n ai-inference

# List models
kubectl exec -n ai-inference <pod> -- ollama list

# Test inference
kubectl exec -n ai-inference <pod> -- ollama run mistral "Hello"

# API call
curl http://ollama.ai-inference.svc:11434/api/generate \
  -d '{"model":"mistral","prompt":"Hi"}'
```

---

## References

- [Ollama Documentation](https://ollama.ai/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama Helm Chart](https://github.com/otwld/ollama-helm)
- [Mistral AI](https://mistral.ai/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [ADR-008: LLM Inference Strategy](../docs/adr/ADR-008-llm-inference-strategy.md)
